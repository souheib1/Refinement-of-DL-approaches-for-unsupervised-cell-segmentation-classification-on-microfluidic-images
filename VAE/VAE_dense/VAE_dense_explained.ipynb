{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center> <h1> Variational Autoencoder using Pytorch </h1> </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "from image_utils import load_images_from_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = (190,105)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We implement the sampling class in order to use the reparameterization trick. In fact, this trick is used in VAEs to sample from the posterior distribution of the latent space variables (z) given the inputs (z_mean, z_log_var) while keeping the sampling process differentiable, enabling backpropagation and gradient-based optimization.\n",
    "\n",
    "The principle is to transform the random samples from a standard normal distribution $ \\sim \\mathcal{N}(0,\\,1)$ using the following equation:\n",
    "        \n",
    "Reparameterization Trick:\n",
    "\n",
    "$$ z = z_{\\text{mean}} + \\exp\\left(\\frac{1}{2} z_{\\text{log\\_var}}\\right) \\cdot \\epsilon $$\n",
    "\n",
    "where $ \\epsilon $ is a random sample from the standard normal distribution $\\sim \\mathcal{N}(0,\\,1)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sampling(nn.Module):\n",
    "    \"\"\"\n",
    "    Implementation of the reparameterization trick (to make the sampling process differentiable) used in VAEs to sample from the\n",
    "    posterior distribution of the latent space variables (z) given the inputs (z_mean, z_log_var).\n",
    "\n",
    "    Parameters:\n",
    "        inputs (tuple): A tuple containing two tensors : z_mean and z_log_var.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: A tensor containing the reparameterized latent variables z.\n",
    "                      The shape of the output tensor is the same as z_mean and z_log_var.\n",
    "    \"\"\"\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        z_mean, z_log_var = inputs\n",
    "        batch = z_mean.shape[0]\n",
    "        dim = z_mean.shape[1]\n",
    "        # generates random samples from a standard normal distribution \n",
    "        epsilon = torch.randn(batch, dim, device=z_mean.device)\n",
    "        #The reparameterization trick\n",
    "        return z_mean + torch.exp(0.5 * z_log_var) * epsilon "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The encoder is a neural network used to map input images of the crops to a triplet (z_mean, z_log_var, z) in the context of a Variational Autoencoder (VAE) architecture. \n",
    "\n",
    " The Encoder takes input image crops and processes them through a series of fully connected layers with ReLU activations to compress the input into a compact representation in the latent space. The architecture consists of three hidden layers with decreasing dimensions, followed by two output layers for the mean and log variance of the latent space distribution. \n",
    "\n",
    "**PS: Idea of using dense layers instead of conv layers**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Encoder neural network that maps input image crops to a triplet (z_mean, z_log_var, z).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_rows=190 ,n_cols=105 ,n_channels=1, h_dim3 = 32, h_dim2 = 64, h_dim1 = 128, latent_dim=32):\n",
    "        super(Encoder, self).__init__()\n",
    "        \n",
    "        self.n_rows = n_rows\n",
    "        self.n_cols = n_cols\n",
    "        self.n_channels = n_channels\n",
    "        self.latent_dim = latent_dim\n",
    "        \n",
    "        self.flatten = nn.Flatten()\n",
    "        self.dense = nn.Linear(n_channels * n_rows * n_cols, h_dim1)\n",
    "        self.fc1 = nn.Linear(h_dim1, h_dim2)\n",
    "        self.fc2 = nn.Linear(h_dim2, h_dim3)\n",
    "        \n",
    "        self.dense_mean = nn.Linear(h_dim3, self.latent_dim)\n",
    "        self.dense_log_var = nn.Linear(h_dim3, self.latent_dim)\n",
    "        self.sampling = Sampling()\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        x = self.flatten(inputs)\n",
    "        x = F.relu(self.dense(x))\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        z_mean = self.dense_mean(x)\n",
    "        z_log_var = self.dense_log_var(x)\n",
    "        z = self.sampling((z_mean, z_log_var))\n",
    "        return z_mean, z_log_var, z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " The decoder architecture maps the latent variables to the original image space by sequentially transforming and upsampling the latent representation through fully connected layers, followed by a final Sigmoid activation to generate the reconstructed image in the original format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Decoder neural network that maps latent variables (z) back to the original image space.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_rows=190 ,n_cols=105 ,n_channels=1, h_dim3 = 32, h_dim2 = 64, h_dim1 = 128, latent_dim=32):\n",
    "        super(Decoder, self).__init__()\n",
    "\n",
    "        self.latent_dim = latent_dim\n",
    "        self.n_rows = n_rows\n",
    "        self.n_cols = n_cols\n",
    "        self.n_channels = n_channels\n",
    "        self.n_pixels = n_rows * n_cols\n",
    "  \n",
    "        \n",
    "\n",
    "        # Decoder layers\n",
    "        self.dense = nn.Linear(self.latent_dim, h_dim3)\n",
    "        self.fc1 = nn.Linear(h_dim3, h_dim2)\n",
    "        self.fc2 = nn.Linear(h_dim2, h_dim1)\n",
    "        self.fc3 = nn.Linear(h_dim1, n_channels * n_rows * n_cols)\n",
    "        \n",
    "\n",
    "    def forward(self, z):\n",
    "        x = F.relu(self.dense(z))\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.sigmoid(self.fc3(x))\n",
    "        return x.view(-1, self.n_channels, self.n_rows, self.n_cols)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. The VAE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BetaVAE(nn.Module):\n",
    "\n",
    "    def __init__(self, beta = 1.0, n_rows=190 ,n_cols=105 ,n_channels=1, h_dim3 = 32, h_dim2 = 64, h_dim1 = 128, latent_dim=32):\n",
    "        super(BetaVAE, self).__init__()\n",
    "\n",
    "        self.encoder = Encoder(n_rows, n_cols, n_channels, h_dim3, h_dim2, h_dim1, latent_dim)\n",
    "        self.sampling = Sampling()\n",
    "        self.decoder = Decoder(n_rows, n_cols, n_channels, h_dim3, h_dim2, h_dim1, latent_dim)\n",
    "        self.beta = beta\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        z_mean, z_log_var, z = self.encoder(inputs)\n",
    "        reconstructed_image = self.decoder(z)\n",
    "        return reconstructed_image,z_mean,z_log_var\n",
    "\n",
    "    def loss_function(self, reconstructed, original, z_mean, z_log_var):\n",
    "\n",
    "        # Reconstruction loss (pixel-wise mean squared error)\n",
    "        reconstruction_loss = F.mse_loss(reconstructed, original, reduction='sum')\n",
    "\n",
    "        # KL-divergence loss\n",
    "        kl_divergence_loss = -0.5 * torch.sum(1 + z_log_var - z_mean.pow(2) - z_log_var.exp())\n",
    "\n",
    "        # Total loss\n",
    "        total_loss = reconstruction_loss + self.beta * kl_divergence_loss\n",
    "\n",
    "        return total_loss, reconstruction_loss, kl_divergence_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_VAE(crops, n_rows=190, n_cols=105, latent_dim=8, beta=1, epochs=20, batch_size=32,\n",
    "                  learning_rate=1e-3, validation_split=0.2, plot_history=True,\n",
    "                  save_model=True, saving_path='./newmodel'):\n",
    "\n",
    "\n",
    "    def __plot_history(history):\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.plot(history['loss'], label='Train Loss')\n",
    "        plt.plot(history['val_loss'], label='Validation Loss')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.legend()\n",
    "        # model_name = \"model_zdim_\" + str(latent_dim)+\"_beta_\"+ str(beta)+\"_epochs_\"+str(epochs)\n",
    "        # plt.savefig('./models/loss_'+model_name+'.png')\n",
    "        plt.show()\n",
    "        \n",
    "\n",
    "    # Convert the crops to a PyTorch tensor\n",
    "    crops = torch.tensor(crops)\n",
    "\n",
    "    # Create an instance of the BetaVAE model\n",
    "    vae = BetaVAE(beta=beta, latent_dim=latent_dim)\n",
    "\n",
    "    # Define the optimizer\n",
    "    optimizer = optim.Adam(vae.parameters(), lr=learning_rate)\n",
    "\n",
    "    # Lists to store loss history for plotting\n",
    "    train_loss_history = []\n",
    "    val_loss_history = []\n",
    "\n",
    "    # Split the data into training and validation sets\n",
    "    split_idx = int((1 - validation_split) * len(crops))\n",
    "    # Shuffle the data before splitting it\n",
    "    size_dim_0 = crops.size(0)\n",
    "    shuffled_indices_dim_0 = torch.randperm(size_dim_0)\n",
    "    crops = crops[shuffled_indices_dim_0, ...]\n",
    "    train_data = crops[:split_idx]\n",
    "    val_data = crops[split_idx:]\n",
    "    print(\"the training data has \",train_data.size(dim=0), \" patches\")\n",
    "    print(\"the validation data has \",val_data.size(dim=0), \" patches\")\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(epochs):\n",
    "        vae.train()\n",
    "        \n",
    "         # Shuffle the training data\n",
    "        dim_0 = train_data.size(0)\n",
    "        shuffled_indices_dim_0 = torch.randperm(dim_0)\n",
    "        train_data = train_data[shuffled_indices_dim_0, ...]\n",
    "        \n",
    "        total_train_loss = 0\n",
    "\n",
    "        for i in range(0, len(train_data), batch_size):\n",
    "            batch = train_data[i:i+batch_size]\n",
    "            optimizer.zero_grad()\n",
    "            reconstructed,z_mean,z_log_var = vae(batch)\n",
    "            loss, reconstruction_loss, kl_divergence_loss = vae.loss_function(\n",
    "                reconstructed, batch, z_mean, z_log_var\n",
    "            )\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_train_loss += loss.item()\n",
    "\n",
    "        avg_train_loss = total_train_loss / len(train_data)\n",
    "        train_loss_history.append(avg_train_loss)\n",
    "\n",
    "        # Validation\n",
    "        vae.eval()\n",
    "        with torch.no_grad():\n",
    "            total_val_loss = 0\n",
    "\n",
    "            for i in range(0, len(val_data), batch_size):\n",
    "                batch = val_data[i:i+batch_size]\n",
    "                reconstructed, z_mean, z_log_var = vae(batch)\n",
    "                loss, _, _ = vae.loss_function(reconstructed, batch, z_mean, z_log_var)\n",
    "                total_val_loss += loss.item()\n",
    "\n",
    "            avg_val_loss = total_val_loss / len(val_data)\n",
    "            val_loss_history.append(avg_val_loss)\n",
    "\n",
    "        print(f\"Epoch [{epoch+1}/{epochs}] - Train Loss: {avg_train_loss:.4f} - Val Loss: {avg_val_loss:.4f}\")\n",
    "\n",
    "    if plot_history:\n",
    "        history = {'loss': train_loss_history, 'val_loss': val_loss_history}\n",
    "        __plot_history(history)\n",
    "\n",
    "    if save_model:\n",
    "        torch.save(vae.state_dict(), saving_path)\n",
    "\n",
    "    return vae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the training data has  5849  patches\n",
      "the validation data has  1463  patches\n",
      "Epoch [1/25] - Train Loss: 554.4177 - Val Loss: 156.4824\n",
      "Epoch [2/25] - Train Loss: 150.5002 - Val Loss: 134.1554\n",
      "Epoch [3/25] - Train Loss: 129.0175 - Val Loss: 114.5244\n",
      "Epoch [4/25] - Train Loss: 117.4513 - Val Loss: 108.3104\n",
      "Epoch [5/25] - Train Loss: 112.9841 - Val Loss: 105.1679\n",
      "Epoch [6/25] - Train Loss: 110.2914 - Val Loss: 102.9130\n",
      "Epoch [7/25] - Train Loss: 108.0615 - Val Loss: 101.9911\n",
      "Epoch [8/25] - Train Loss: 106.4609 - Val Loss: 100.9605\n",
      "Epoch [9/25] - Train Loss: 105.1266 - Val Loss: 100.3471\n",
      "Epoch [10/25] - Train Loss: 104.1163 - Val Loss: 97.7357\n",
      "Epoch [11/25] - Train Loss: 102.8283 - Val Loss: 97.1675\n",
      "Epoch [12/25] - Train Loss: 102.3020 - Val Loss: 97.2780\n",
      "Epoch [13/25] - Train Loss: 101.2529 - Val Loss: 95.5602\n",
      "Epoch [14/25] - Train Loss: 100.6333 - Val Loss: 96.5246\n",
      "Epoch [15/25] - Train Loss: 100.0476 - Val Loss: 94.1845\n",
      "Epoch [16/25] - Train Loss: 99.2792 - Val Loss: 94.3981\n",
      "Epoch [17/25] - Train Loss: 98.6946 - Val Loss: 94.0717\n",
      "Epoch [18/25] - Train Loss: 97.9471 - Val Loss: 92.5544\n",
      "Epoch [19/25] - Train Loss: 97.4280 - Val Loss: 92.2740\n",
      "Epoch [20/25] - Train Loss: 97.3501 - Val Loss: 92.8007\n",
      "Epoch [21/25] - Train Loss: 96.7635 - Val Loss: 92.0721\n",
      "Epoch [22/25] - Train Loss: 96.5682 - Val Loss: 91.4076\n",
      "Epoch [23/25] - Train Loss: 96.2711 - Val Loss: 91.3840\n",
      "Epoch [24/25] - Train Loss: 95.9680 - Val Loss: 91.2470\n",
      "Epoch [25/25] - Train Loss: 95.8540 - Val Loss: 91.3592\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "data_path = '../segmentation/data_for_VAE/220429_ MCF10A  laminAC fibro phallo pattern mars 2022\\*.png'\n",
    "_,crops = load_images_from_path(data_path)\n",
    "crops = np.expand_dims(crops, axis=1).astype(\"float32\") / 255\n",
    "batch_size = 32  # You can adjust this based on your hardware and memory limitations\n",
    "latent_dim = 8\n",
    "beta = 1\n",
    "epochs = 25\n",
    "model_name = \"model_zdim_\" + str(latent_dim)+\"_beta_\"+ str(beta)+\"_epochs_\"+str(epochs)\n",
    "trained_vae = train_VAE(crops, n_rows=190, n_cols=105, latent_dim=latent_dim, beta=beta, epochs=epochs,\n",
    "                        batch_size=batch_size, learning_rate=1e-3, validation_split=0.2,\n",
    "                        plot_history=True, save_model=True, saving_path='./models/'+model_name+\".pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BetaVAE(\n",
       "  (encoder): Encoder(\n",
       "    (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "    (dense): Linear(in_features=19950, out_features=128, bias=True)\n",
       "    (fc1): Linear(in_features=128, out_features=64, bias=True)\n",
       "    (fc2): Linear(in_features=64, out_features=32, bias=True)\n",
       "    (dense_mean): Linear(in_features=32, out_features=8, bias=True)\n",
       "    (dense_log_var): Linear(in_features=32, out_features=8, bias=True)\n",
       "    (sampling): Sampling()\n",
       "  )\n",
       "  (sampling): Sampling()\n",
       "  (decoder): Decoder(\n",
       "    (dense): Linear(in_features=8, out_features=32, bias=True)\n",
       "    (fc1): Linear(in_features=32, out_features=64, bias=True)\n",
       "    (fc2): Linear(in_features=64, out_features=128, bias=True)\n",
       "    (fc3): Linear(in_features=128, out_features=19950, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = BetaVAE(beta=1, latent_dim=8)\n",
    "model.load_state_dict(torch.load('./models/model_zdim_8_beta_1_epochs_50.pth'))\n",
    "model.eval()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "allwhatyouneed",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
